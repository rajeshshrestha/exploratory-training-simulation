{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import pickle as pk\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>rating</th>\n",
       "      <th>genre</th>\n",
       "      <th>director</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Secret Life of Walter Mitty</td>\n",
       "      <td>2013</td>\n",
       "      <td>PG</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>Ben Stiller</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Secret</td>\n",
       "      <td>2013</td>\n",
       "      <td>R</td>\n",
       "      <td>Crime, Drama, Thriller</td>\n",
       "      <td>Charlie Stratton</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Giver</td>\n",
       "      <td>2014</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>Drama, Sci-Fi</td>\n",
       "      <td>Phillip Noyce</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>2013</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>Biography, Drama, Sport</td>\n",
       "      <td>Brian Helgeland</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>House at the End of the Drive</td>\n",
       "      <td>2014</td>\n",
       "      <td>R</td>\n",
       "      <td>Horror, Thriller</td>\n",
       "      <td>David Worth</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  year rating                     genre  \\\n",
       "0  The Secret Life of Walter Mitty  2013     PG  Adventure, Comedy, Drama   \n",
       "1                        In Secret  2013      R    Crime, Drama, Thriller   \n",
       "2                        The Giver  2014  PG-13             Drama, Sci-Fi   \n",
       "3                               42  2013  PG-13   Biography, Drama, Sport   \n",
       "4    House at the End of the Drive  2014      R          Horror, Thriller   \n",
       "\n",
       "           director language country   type  \n",
       "0       Ben Stiller  English     USA  movie  \n",
       "1  Charlie Stratton  English     USA  movie  \n",
       "2     Phillip Noyce  English     USA  movie  \n",
       "3   Brian Helgeland  English     USA  movie  \n",
       "4       David Worth  English     USA  movie  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"./data/preprocess-data/omdb-clean-full.csv\"\n",
    "\n",
    "raw_df = pd.read_csv(data_path)\n",
    "raw_df.rename(columns=dict((col, col.lower()) for col in raw_df.columns), inplace=True)\n",
    "raw_df.index = raw_df.index.map(str)\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Read Scenario3 info for the functional dependencies\n",
    "\n",
    "with open(\"./scenarios.json\", 'r') as fp:\n",
    "    scenarios = json.load(fp)\n",
    "required_scenario_info = scenarios[\"3\"]\n",
    "hypothesis_space = [hypothesis['cfd'] for hypothesis in required_scenario_info['hypothesis_space']]\n",
    "print(hypothesis_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add info to new scenarios dict and dump the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hypothesis(fd):\n",
    "    lfd, rfd = fd.split(\"=>\")\n",
    "\n",
    "    '''Parse left fd and separate out the attributes'''\n",
    "    left_attributes = lfd.strip().strip(\"(\").strip(\")\").split(\",\")\n",
    "    right_attributes = rfd.strip(\"(\").strip(\")\").split(\",\")\n",
    "\n",
    "    left_attributes = [attribute.strip() for attribute in left_attributes]\n",
    "    right_attributes = [attribute.strip() for attribute in right_attributes]\n",
    "\n",
    "    return left_attributes, right_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_support_violation(fd_components, tuple_1, tuple_2):\n",
    "    '''Parse the hypothesis'''\n",
    "    lfd, rfd = fd_components\n",
    "\n",
    "    '''Violation check is only needed if the lfd values are same in both tuples otherwise it's not a violation'''\n",
    "    is_left_same = all(tuple_1[left_attribute] == tuple_2[left_attribute] for left_attribute in lfd)\n",
    "\n",
    "    if is_left_same:\n",
    "        is_right_same = all(tuple_1[left_attribute] == tuple_2[left_attribute] for left_attribute in rfd)\n",
    "        if is_right_same:\n",
    "            return True, False\n",
    "        else:\n",
    "            return False, True\n",
    "    else:\n",
    "        return False, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_support_violation_tuples(data, idx, fd_components):\n",
    "    supports = []\n",
    "    violations = []\n",
    "    for idx_ in data.index:\n",
    "        if idx == idx_:\n",
    "            continue\n",
    "        is_support, is_violation = is_support_violation(fd_components=fd_components, tuple_1=data.iloc[idx], tuple_2=data.iloc[idx_])\n",
    "        if is_support:\n",
    "            supports.append(idx_)\n",
    "        elif is_violation:\n",
    "            violations.append(idx_)\n",
    "    return supports, violations\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypothesis_info_dict(hypothesis):\n",
    "    '''Extract left and right attributes from the hypothesis as list of attributes'''\n",
    "    lfd, rfd = parse_hypothesis(hypothesis)\n",
    "    info_dict = {'lfd': lfd, 'rfd':rfd}\n",
    "\n",
    "    '''Find pairwise violations of each tuple with respect to other tuples in the dataset'''\n",
    "    info_dict['supports'] = dict()\n",
    "    info_dict['violations'] = dict()\n",
    "    for idx in tqdm(raw_df.index):\n",
    "        supports, violations = get_support_violation_tuples(data=raw_df[lfd+rfd], idx=idx, fd_components=(lfd, rfd))\n",
    "        info_dict['supports'][idx] = supports\n",
    "        info_dict['violations'][idx] = violations\n",
    "    \n",
    "    return info_dict\n",
    "\n",
    "# get_hypothesis_info_dict(hypothesis_space[0])\n",
    "# hypothesis_space[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_num = os.cpu_count()\n",
    "cpu_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hypothesis_space' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m8/p1693_l938l9s_6f9h2nhnxh0000gn/T/ipykernel_22649/2669582920.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpu_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhypothesis_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_hypothesis_info_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hypothesis_space' is not defined"
     ]
    }
   ],
   "source": [
    "new_scenarios_dict = dict()\n",
    "new_scenarios_dict['omdb'] = dict()\n",
    "new_scenarios_dict['omdb']['hypothesis_space'] = dict()\n",
    "\n",
    "\n",
    "with Pool(cpu_num) as p:\n",
    "    hypothesis_info = p.map(get_hypothesis_info_dict, hypothesis_space)\n",
    "\n",
    "for hypothesis, info_dict in zip(hypothesis_space, hypothesis_info):\n",
    "    new_scenarios_dict['omdb']['hypothesis_space'][hypothesis] = info_dict\n",
    "\n",
    "# for hypothesis in tqdm(hypothesis_space):\n",
    "    \n",
    "    # '''Extract left and right attributes from the hypothesis as list of attributes'''\n",
    "    # lfd, rfd = parse_hypothesis(hypothesis)\n",
    "    # new_scenarios_dict['omdb']['hypothesis_space'][hypothesis] = {'lfd': lfd, 'rfd':rfd}\n",
    "\n",
    "    # '''Find pairwise violations of each tuple with respect to other tuples in the dataset'''\n",
    "    # new_scenarios_dict['omdb']['hypothesis_space'][hypothesis]['violations'] = dict()\n",
    "    # for idx in raw_df.index:\n",
    "    #     new_scenarios_dict['omdb']['hypothesis_space'][hypothesis]['violations'][idx]=get_violation_tuples(data=raw_df, idx=idx, fd=hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in new_scenarios_dict:\n",
    "    new_scenarios_dict[dataset]['data_indices'] = [str(x) for x in raw_df.index]\n",
    "    for hypothesis in new_scenarios_dict[dataset]['hypothesis_space']:\n",
    "        for val_type in ['supports', 'violations']:\n",
    "            for idx in new_scenarios_dict[dataset]['hypothesis_space'][hypothesis][val_type]:\n",
    "\n",
    "                if int(idx) in new_scenarios_dict[dataset]['hypothesis_space'][hypothesis][val_type][idx]:\n",
    "                    new_scenarios_dict[dataset]['hypothesis_space'][hypothesis][val_type][idx].remove(int(idx))\n",
    "\n",
    "                '''Don't assign if the list contains self index'''\n",
    "                if new_scenarios_dict[dataset]['hypothesis_space'][hypothesis][val_type][idx] != []:\n",
    "                    new_scenarios_dict[dataset]['hypothesis_space'][hypothesis][val_type][idx] = [str(x) for x in new_scenarios_dict[dataset]['hypothesis_space'][hypothesis][val_type][idx] if str(x)!=str(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scenarios_dict['omdb']['processed_dataset_path'] = \"data/processed-data/omdb-sampled.csv\"\n",
    "new_scenarios_dict['omdb']['raw_dataset_path'] = \"data/preprocess-data/omdb-clean-full.csv\"\n",
    "\n",
    "with open(\"./new_scenarios.json\", 'w') as fp:\n",
    "    json.dump(new_scenarios_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./new_scenarios.json\", 'w') as fp:\n",
    "#     json.dump(new_scenarios_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read new scenarios file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./new_scenarios.json\", 'r') as fp:\n",
    "    new_scenarios_dict = json.load(fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_scenarios_dict['omdb']['hypothesis_space'][hypothesis_space[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_support_violation_ratio_info = dict()\n",
    "for hypothesis in new_scenarios_dict['omdb']['hypothesis_space']:\n",
    "    hypothesis_info_dict = new_scenarios_dict['omdb']['hypothesis_space'][hypothesis]\n",
    "    if len(hypothesis_info_dict['lfd']+hypothesis_info_dict['rfd']) not in [3,4]:\n",
    "        continue\n",
    "    \n",
    "    support_pairs_num, violation_pairs_num = 0,0\n",
    "    for idx in hypothesis_info_dict['supports']:\n",
    "        support_pairs_num += len(hypothesis_info_dict['supports'][idx])\n",
    "\n",
    "    for idx in hypothesis_info_dict['violations']:\n",
    "        violation_pairs_num += len(hypothesis_info_dict['violations'][idx])\n",
    "    \n",
    "    hypothesis_support_violation_ratio_info[hypothesis] = support_pairs_num/(support_pairs_num+violation_pairs_num)\n",
    "# pprint(hypothesis_support_violation_ratio_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sample confidence from 0 to support_violation_ratio'''\n",
    "np.random.seed(1000)\n",
    "model = dict((hypothesis, np.random.uniform(max(0,ratio-0.25), min(1,ratio+0.25) )) for hypothesis, ratio in hypothesis_support_violation_ratio_info.items())\n",
    "model_dict={'omdb':{'model':model}}\n",
    "# model = dict((hypothesis, ratio ) for hypothesis, ratio in hypothesis_support_violation_ratio_info.items())\n",
    "\n",
    "# pprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./trainer_model.json\", 'w') as fp:\n",
    "    json.dump(model_dict, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Predict a tuple using a model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def predict_clean_tuple(idx, model):\n",
    "    total_score = 0\n",
    "    for hypothesis, conf in model.items():\n",
    "        '''Check the number of supports and violations based on the model'''\n",
    "        support_pairs_num = len(new_scenarios_dict['omdb']['hypothesis_space'][hypothesis]['supports'].get(idx, []))\n",
    "        violation_pairs_num = len(new_scenarios_dict['omdb']['hypothesis_space'][hypothesis]['violations'].get(idx, []))\n",
    "\n",
    "        '''Vote according to the hypothesis conf'''\n",
    "        total_score += (conf*(support_pairs_num-violation_pairs_num))\n",
    "    \n",
    "    if total_score > 0:\n",
    "        return True, total_score\n",
    "    \n",
    "    else:\n",
    "        return False, total_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clean_tuple_indices = set()\n",
    "model_score_dict = dict()\n",
    "for idx in new_scenarios_dict['omdb']['data_indices']:         \n",
    "    is_clean, total_score = predict_clean_tuple(idx, model)\n",
    "    if is_clean:\n",
    "        clean_tuple_indices.add(idx)\n",
    "    model_score_dict[idx] = total_score\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(clean_tuple_indices)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_dict['omdb']['predictions'] =  dict((idx, True) if idx in clean_tuple_indices else (idx,False) for idx in new_scenarios_dict['omdb']['data_indices'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(\"./trainer_model.json\", 'w') as fp:\n",
    "    json.dump(model_dict, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0.9**10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the coditional probability of a tuple being cleaned conditional to all the tuples being clean\n",
    "- Let's suppose t1 has compliance and violations with t2, t3 and t4 only. Then the conditional probability becomes independent of other variables\n",
    "- P(t1=C|t2=C,t3=C,......) = P(t1=C|t2=C,t3=C,t4=C)\n",
    "    - = P(t1=C, t2=C, t3=C, t4=C)/P(t2=C, t3=C, t4=C)\n",
    "    - = P(t1=C, t2=C, t3=C, t4=C)/(P(t1=C, t2=C, t3=C, t4=C) + P(t1=D, t2=C, t3=C, t4=C))\n",
    "    - = 1/Z\\*exp(p*(#ofcompliance(t1=C, t2=C, t3=C, t4=C)-#ofviolations_(t1=C, t2=C, t3=C, t4=C)))/[1/Z*(exp(p*(#ofcompliance(t1=C, t2=C, t3=C, t4=C)-#ofviolations_(t1=C, t2=C, t3=C, t4=C)))+(exp(p*(#ofcompliance(t1=D, t2=C, t3=C, t4=C)-#ofviolations_(t1=D, t2=C, t3=C, t4=C))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_clean_prob(idx, fd, model_probab, valid_indices = None):\n",
    "    if valid_indices is None:\n",
    "        compliance_num = len(new_scenarios_dict['omdb']['hypothesis_space'][fd]['supports'].get(str(idx), []))\n",
    "        violation_num = len(new_scenarios_dict['omdb']['hypothesis_space'][fd]['violations'].get(str(idx),[]))\n",
    "    else:\n",
    "        compliance_num = len([idx_ for idx_ in new_scenarios_dict['omdb']['hypothesis_space'][fd]['supports'].get(str(idx), []) if idx_ in valid_indices])\n",
    "        violation_num = len([idx_ for idx_ in new_scenarios_dict['omdb']['hypothesis_space'][fd]['violations'].get(str(idx),[]) if idx_ in valid_indices])\n",
    "\n",
    "    tuple_clean_score = math.exp(model_probab*(compliance_num-violation_num))\n",
    "    tuple_dirty_score = math.exp(model_probab*(-compliance_num+violation_num))\n",
    "    cond_p_clean = tuple_clean_score/(tuple_clean_score+tuple_dirty_score)\n",
    "    return cond_p_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2403 890\n",
      "1650\n"
     ]
    }
   ],
   "source": [
    "model = model_dict['omdb']['model']\n",
    "conditional_clean_probability_dict = dict()\n",
    "clean_indices = set()\n",
    "dirty_indices = set()\n",
    "\n",
    "clean_max_num = 1500\n",
    "dirty_sample_percentage = 0.1\n",
    "\n",
    "data_indices = new_scenarios_dict['omdb']['data_indices']\n",
    "\n",
    "top_10_fds = dict(sorted(model.items(), key=itemgetter(1), reverse=True)[:10])\n",
    "\n",
    "for idx in data_indices:\n",
    "    conditional_clean_probability_dict[idx] = {'hypothesis':dict()}\n",
    "    for fd, model_probab in top_10_fds.items():\n",
    "        conditional_clean_probability_dict[idx]['hypothesis'][fd] = get_conditional_clean_prob(idx, fd, model_probab=model_probab)\n",
    "    conditional_clean_probability_dict[idx]['average'] = np.mean(list(conditional_clean_probability_dict[idx]['hypothesis'].values()))\n",
    "    is_idx_clean = conditional_clean_probability_dict[idx]['average']>=0.5\n",
    "    conditional_clean_probability_dict[idx]['is_clean'] = is_idx_clean\n",
    "\n",
    "    if is_idx_clean:\n",
    "        clean_indices.add(idx)\n",
    "    else:\n",
    "        dirty_indices.add(idx)\n",
    "else:\n",
    "    # pprint(conditional_clean_probability_dict)\n",
    "    print(len(clean_indices), len(dirty_indices))\n",
    "\n",
    "clean_sample_idxs = np.random.choice(list(clean_indices), min(len(clean_indices), clean_max_num), replace=False)\n",
    "dirty_sample_idxs = np.random.choice(list(dirty_indices), int(dirty_sample_percentage*len(clean_sample_idxs)), replace=False)\n",
    "sampled_data_indices = set(clean_sample_idxs).union(set(dirty_sample_idxs))\n",
    "print(len(sampled_data_indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun the model computation and is_clean prediction using this computed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Data Number: 1529,Dirty Data Number: 121,Dirty Data Proportion: 0.07333333333333333\n",
      "MAE: 8.596094074371745\n",
      "Clean Data Number: 1548,Dirty Data Number: 102,Dirty Data Proportion: 0.06181818181818182\n",
      "MAE: 0.3976471601897835\n",
      "Clean Data Number: 1548,Dirty Data Number: 102,Dirty Data Proportion: 0.06181818181818182\n",
      "MAE: 0.0\n"
     ]
    }
   ],
   "source": [
    "'''Assume every data to be clean at the beginning and compute the model based on that'''\n",
    "new_model = deepcopy(model_dict['omdb']['model'])\n",
    "model_mae = float(\"inf\")\n",
    "\n",
    "new_data_indices = sampled_data_indices\n",
    "\n",
    "while model_mae > 1e-05:\n",
    "\n",
    "    '''Use current model to predict clean and dirty indices'''\n",
    "    top_10_fds = dict(\n",
    "        sorted(new_model.items(), key=itemgetter(1), reverse=True)[:10])\n",
    "\n",
    "    new_conditional_clean_probability_dict = dict()\n",
    "    new_clean_indices = set()\n",
    "    new_dirty_indices = set()\n",
    "\n",
    "    for idx in new_data_indices:\n",
    "        new_conditional_clean_probability_dict[idx] = {'hypothesis': dict()}\n",
    "        for fd, model_probab in top_10_fds.items():\n",
    "            new_conditional_clean_probability_dict[idx]['hypothesis'][fd] = get_conditional_clean_prob(\n",
    "                idx, fd, model_probab=model_probab, valid_indices=new_data_indices)\n",
    "        new_conditional_clean_probability_dict[idx]['average'] = np.mean(\n",
    "            list(new_conditional_clean_probability_dict[idx]['hypothesis'].values()))\n",
    "        is_idx_clean = new_conditional_clean_probability_dict[idx]['average'] >= 0.5\n",
    "        new_conditional_clean_probability_dict[idx]['is_clean'] = is_idx_clean\n",
    "\n",
    "        if is_idx_clean:\n",
    "            new_clean_indices.add(idx)\n",
    "        else:\n",
    "            new_dirty_indices.add(idx)\n",
    "\n",
    "    else:\n",
    "        # pprint(new_conditional_clean_probability_dict)\n",
    "        print(f\"Clean Data Number: {len(new_clean_indices)},\"\n",
    "              f\"Dirty Data Number: {len(new_dirty_indices)},\"\n",
    "              f\"Dirty Data Proportion: {len(new_dirty_indices)/len(new_clean_indices.union(new_dirty_indices))}\")\n",
    "\n",
    "    '''Use clean data to estimate model'''\n",
    "    model_mae = 0\n",
    "    for hypothesis in new_scenarios_dict['omdb']['hypothesis_space']:\n",
    "        hypothesis_info_dict = new_scenarios_dict['omdb']['hypothesis_space'][hypothesis]\n",
    "        if len(hypothesis_info_dict['lfd']+hypothesis_info_dict['rfd']) not in [3, 4]:\n",
    "            continue\n",
    "\n",
    "        '''Only consider the clean estimated indices'''\n",
    "        support_pairs_num, violation_pairs_num = 0, 0\n",
    "        for idx in hypothesis_info_dict['supports']:\n",
    "            if idx not in new_clean_indices:\n",
    "                continue\n",
    "            support_pairs_num += len([idx1 for idx1 in hypothesis_info_dict['supports']\n",
    "                                     [idx] if idx1 in new_clean_indices])\n",
    "\n",
    "        for idx in hypothesis_info_dict['violations']:\n",
    "            if idx not in new_clean_indices:\n",
    "                continue\n",
    "            violation_pairs_num += len(\n",
    "                [idx1 for idx1 in hypothesis_info_dict['violations'][idx] if idx1 in new_clean_indices])\n",
    "\n",
    "        fd_prob = support_pairs_num/(support_pairs_num+violation_pairs_num)\n",
    "\n",
    "        '''Compute mae with previous model value'''\n",
    "        model_mae += abs(new_model[hypothesis]-fd_prob)\n",
    "        new_model[hypothesis] = fd_prob\n",
    "\n",
    "    print(f\"MAE: {model_mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_max_num = 1500\n",
    "# dirty_sample_percentage = 0.1\n",
    "\n",
    "# clean_sample_idxs = np.random.choice(list(new_clean_indices), min(len(new_clean_indices), clean_max_num), replace=False)\n",
    "# dirty_sample_idxs = np.random.choice(list(new_dirty_indices), int(dirty_sample_percentage*len(clean_sample_idxs)), replace=False)\n",
    "# # sampled_data_indices = set(clean_sample_idxs).union(set(dirty_sample_idxs))\n",
    "# sampled_data_indices = new_clean_indices.union(new_dirty_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict={'omdb':{'model':new_model}}\n",
    "model_dict['omdb']['predictions'] =  dict((idx, True) if idx in new_clean_indices else (idx,False) for idx in new_data_indices)\n",
    "with open(\"./trainer_model.json\", 'w') as fp:\n",
    "    json.dump(model_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>rating</th>\n",
       "      <th>genre</th>\n",
       "      <th>director</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>We're Done</td>\n",
       "      <td>2014</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>Comedy, Drama</td>\n",
       "      <td>Cherie Nowlan</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>episode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>Battles Bouts and Brawls: The Story of Pro Wre...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Not Rated</td>\n",
       "      <td>Sport</td>\n",
       "      <td>Mark Nowotarski</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>World's Busiest Border Crossing</td>\n",
       "      <td>2013</td>\n",
       "      <td>TV-G</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>Eoin O'Shea</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>The Armstrong Lie</td>\n",
       "      <td>2013</td>\n",
       "      <td>R</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>Alex Gibney</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>Jaz</td>\n",
       "      <td>2013</td>\n",
       "      <td>Unrated</td>\n",
       "      <td>Short, Music</td>\n",
       "      <td>Greg W. Locke</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>By the Gun</td>\n",
       "      <td>2014</td>\n",
       "      <td>R</td>\n",
       "      <td>Crime, Drama, Thriller</td>\n",
       "      <td>James Mottern</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>Big Episode: Someone Stole a Spoon</td>\n",
       "      <td>2013</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>James Widdoes</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>episode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>The Gift</td>\n",
       "      <td>2013</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Gail Mancuso</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>episode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>Oconomowoc</td>\n",
       "      <td>2013</td>\n",
       "      <td>Not Rated</td>\n",
       "      <td>Comedy, Drama</td>\n",
       "      <td>Andy Gillies</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>Off the Record</td>\n",
       "      <td>2013</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>Drama, Thriller</td>\n",
       "      <td>Duane Clark</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>episode</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1650 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  year     rating  \\\n",
       "3212                                         We're Done  2014      TV-14   \n",
       "3166  Battles Bouts and Brawls: The Story of Pro Wre...  2014  Not Rated   \n",
       "1727                    World's Busiest Border Crossing  2013       TV-G   \n",
       "126                                   The Armstrong Lie  2013          R   \n",
       "1145                                                Jaz  2013    Unrated   \n",
       "...                                                 ...   ...        ...   \n",
       "798                                          By the Gun  2014          R   \n",
       "1338                 Big Episode: Someone Stole a Spoon  2013      TV-14   \n",
       "2189                                           The Gift  2013      TV-14   \n",
       "391                                          Oconomowoc  2013  Not Rated   \n",
       "2431                                     Off the Record  2013      TV-14   \n",
       "\n",
       "                       genre         director language country     type  \n",
       "3212           Comedy, Drama    Cherie Nowlan  English     USA  episode  \n",
       "3166                   Sport  Mark Nowotarski  English     USA    movie  \n",
       "1727             Documentary      Eoin O'Shea  English     USA    movie  \n",
       "126              Documentary      Alex Gibney  English     USA    movie  \n",
       "1145            Short, Music    Greg W. Locke  English     USA    movie  \n",
       "...                      ...              ...      ...     ...      ...  \n",
       "798   Crime, Drama, Thriller    James Mottern  English     USA    movie  \n",
       "1338                  Comedy    James Widdoes  English     USA  episode  \n",
       "2189                  Comedy     Gail Mancuso  English     USA  episode  \n",
       "391            Comedy, Drama     Andy Gillies  English     USA    movie  \n",
       "2431         Drama, Thriller      Duane Clark  English     USA  episode  \n",
       "\n",
       "[1650 rows x 8 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df = raw_df.loc[list(new_data_indices)]\n",
    "# sampled_df['is_clean'] = sampled_df.index.map(lambda x: model_dict['omdb']['predictions'][x])\n",
    "# del sampled_df['is_clean']\n",
    "sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./data/processed-data\", exist_ok=True)\n",
    "sampled_df.to_csv(\"./data/processed-data/omdb-sampled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lfd', 'rfd', 'supports', 'violations'])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_scenarios_dict['omdb']['hypothesis_space']['(title) => director'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Process and Dump pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  8.10it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('./trainer_model.json', 'r') as f:\n",
    "    models_dict = json.load(f)\n",
    "\n",
    "required_fds = dict(\n",
    "    (scenario, set(models_dict[scenario]['model'].keys())) for scenario in models_dict)\n",
    "\n",
    "with open(\"./data/processed-exp-data/trainer_model.json\", 'w') as fp:\n",
    "    json.dump(models_dict, fp)\n",
    "\n",
    "with open(\"./data/processed-exp-data/required_fds.pk\", 'wb') as fp:\n",
    "    pk.dump(required_fds, fp)\n",
    "\n",
    "\n",
    "with open('./new_scenarios.json', 'r') as f:\n",
    "    scenarios = json.load(f)\n",
    "\n",
    "'''Process new_scenarios to make the processing faster later'''\n",
    "processed_df = dict()\n",
    "filtered_processed_scenarios = dict()\n",
    "for dataset in scenarios:\n",
    "\n",
    "    processed_df[dataset] = pd.read_csv(\n",
    "        scenarios['omdb']['processed_dataset_path'], index_col=0)\n",
    "    processed_df[dataset].index = processed_df[dataset].index.map(str)\n",
    "    required_indices = set(processed_df[dataset].index)\n",
    "\n",
    "    filtered_processed_scenarios= {dataset:{'data_indices': set(\n",
    "        scenarios[dataset]['data_indices']).intersection(required_indices), 'hypothesis_space': dict()}}\n",
    "\n",
    "    '''Filter required fds and data_indices'''\n",
    "    for hypothesis in tqdm(scenarios[dataset]['hypothesis_space']):\n",
    "        if hypothesis not in required_fds[dataset]:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        filtered_processed_scenarios[dataset]['hypothesis_space'][hypothesis]={'lfd':set(\n",
    "            scenarios[dataset]['hypothesis_space'][hypothesis]['lfd']),\n",
    "        'rfd': set(\n",
    "            scenarios[dataset]['hypothesis_space'][hypothesis]['rfd'])}\n",
    "\n",
    "        for info_type in ['supports', 'violations']:\n",
    "            filtered_processed_scenarios[dataset]['hypothesis_space'][hypothesis][info_type] = dict()\n",
    "\n",
    "            filtered_processed_scenarios[dataset]['hypothesis_space'][hypothesis][f'{info_type[:-1]}_pairs'] = set()\n",
    "            for idx in scenarios[dataset]['hypothesis_space'][hypothesis][info_type]:\n",
    "                if idx not in required_indices:\n",
    "                    continue\n",
    "\n",
    "                filtered_processed_scenarios[dataset]['hypothesis_space'][hypothesis][info_type][idx] = set(\n",
    "                    scenarios[dataset]['hypothesis_space'][hypothesis][info_type][idx]).intersection(required_indices)\n",
    "                \n",
    "                pairs = set((idx, idx_) if idx<idx_ else (idx_, idx) for idx_ in filtered_processed_scenarios[dataset]['hypothesis_space'][hypothesis][info_type][idx])\n",
    "                filtered_processed_scenarios[dataset]['hypothesis_space'][hypothesis][f'{info_type[:-1]}_pairs'] |= pairs\n",
    "\n",
    "\n",
    "with open(\"./data/processed-exp-data/filtered_processed_scenarios.pk\", 'wb') as fp:\n",
    "    pk.dump(filtered_processed_scenarios, fp)\n",
    "\n",
    "with open(\"./data/processed-exp-data/processed_dfs.pk\", 'wb') as fp:\n",
    "    pk.dump(processed_df, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data_indices', 'hypothesis_space'])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_processed_scenarios['omdb'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle as pk\n",
    "from statistics import mean\n",
    "from operator import itemgetter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./trainer_model.json', 'r') as f:\n",
    "    _models_dict = json.load(f)\n",
    "\n",
    "with open(\"./data/processed-exp-data/filtered_processed_scenarios.pk\", 'rb') as fp:\n",
    "    _filtered_processed_scenarios = pk.load(fp)\n",
    "\n",
    "with open(\"./data/processed-exp-data/processed_dfs.pk\", 'rb') as fp:\n",
    "    _processed_df = pk.load(fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_clean_prob(idx, fd, fd_prob, scenario_id, data_indices=None):\n",
    "    if data_indices is None:\n",
    "        compliance_num = len(\n",
    "            _filtered_processed_scenarios[scenario_id]['hypothesis_space'][fd]['supports'].get(idx, []))\n",
    "        violation_num = len(\n",
    "            _filtered_processed_scenarios[scenario_id]['hypothesis_space'][fd]['violations'].get(idx, []))\n",
    "    else:\n",
    "        compliance_num = len([idx_ for idx_ in _filtered_processed_scenarios[scenario_id]['hypothesis_space']\n",
    "                                [fd]['supports'].get(idx, [])\n",
    "                                if idx_ in data_indices])\n",
    "        violation_num = len([idx_ for idx_ in _filtered_processed_scenarios[scenario_id]['hypothesis_space']\n",
    "                            [fd]['violations'].get(idx, []) if idx_ in data_indices])\n",
    "\n",
    "    tuple_clean_score = math.exp(fd_prob*(compliance_num-violation_num))\n",
    "    tuple_dirty_score = math.exp(fd_prob*(-compliance_num+violation_num))\n",
    "    cond_p_clean = tuple_clean_score/(tuple_clean_score+tuple_dirty_score)\n",
    "\n",
    "    return cond_p_clean\n",
    "\n",
    "def get_average_cond_clean_prediction(indices, model, scenario_id):\n",
    "    conditional_clean_probability_dict = dict()\n",
    "    indices = set(indices)\n",
    "    for idx in indices:\n",
    "        cond_clean_prob = mean([compute_conditional_clean_prob(\n",
    "            idx=idx, fd=fd, fd_prob=fd_prob, scenario_id=scenario_id,\n",
    "            data_indices=indices) for fd, fd_prob in model.items()])  # whether to include the validation_indices or all the data_indices while computing the conditional clean probability\n",
    "        conditional_clean_probability_dict[idx] = cond_clean_prob\n",
    "    return conditional_clean_probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_model = dict(sorted(_models_dict['omdb']['model'].items(), key=itemgetter(1),\n",
    "                    reverse=True)[:10])\n",
    "_clean_indices = set([idx for idx in _models_dict['omdb']['predictions'] if _models_dict['omdb']['predictions'][idx]])\n",
    "_clean_probab_dict = get_average_cond_clean_prediction(_processed_df['omdb'].index, model=_model, scenario_id='omdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the clean label in the model file\n",
    "for idx in _processed_df['omdb'].index:\n",
    "    _clean = _clean_probab_dict[idx] >= 0.5\n",
    "    if _clean == _models_dict['omdb']['predictions'][idx]:\n",
    "        continue\n",
    "    print(idx, _clean, _models_dict['omdb']['predictions'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the aggreage model on the overall data\n",
    "for hypothesis in _models_dict['omdb']['model']:\n",
    "    hypothesis_info_dict = _filtered_processed_scenarios['omdb']['hypothesis_space'][hypothesis]\n",
    "\n",
    "    support_pairs_num, violation_pairs_num = 0,0\n",
    "    for idx in hypothesis_info_dict['supports']:\n",
    "        if idx not in _clean_indices:\n",
    "            continue\n",
    "        support_pairs_num += len(set(hypothesis_info_dict['supports'][idx]).intersection(_clean_indices))\n",
    "\n",
    "    for idx in hypothesis_info_dict['violations']:\n",
    "        if idx not in _clean_indices:\n",
    "            continue\n",
    "        violation_pairs_num += len(set(hypothesis_info_dict['violations'][idx]).intersection(_clean_indices))\n",
    "    \n",
    "    is_correct = (support_pairs_num/(support_pairs_num+violation_pairs_num)) == _models_dict['omdb']['model'][hypothesis]\n",
    "    \n",
    "    if not is_correct:\n",
    "        print((support_pairs_num/(support_pairs_num+violation_pairs_num)),  _models_dict['omdb']['model'][hypothesis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "validation_indices = {'omdb': sample(list(_models_dict['omdb']['predictions'].keys()), 1000)}\n",
    "\n",
    "with open('./data/processed-data/validation_indices.json','w') as fp:\n",
    "    json.dump(validation_indices, fp)\n",
    "\n",
    "validation_indices['omdb'] = set(validation_indices['omdb'])\n",
    "\n",
    "with open('./data/processed-exp-data/validation_indices.pk','wb') as fp:\n",
    "    pk.dump(validation_indices, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8 (default, Nov 16 2020, 16:55:22) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cee6b893c677ec503583163a9ebb4864f4c9dcdccf790e78343065dd69043324"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
